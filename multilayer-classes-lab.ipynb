{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vícevrstvé sítě a zpětná propagace\n",
    "\n",
    "V tomto cvičení si vyzkoušíme klasifikaci pomocí vícevrstvých dopředných sítí, tzv. vícevrstvých perceptronů. Pro porovnání se přitom zaměříme na stejný dataset jako minule, tedy klasifikaci koček."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12., 8.)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Načtení dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trénovací a validační množiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "X_all = np.zeros((50000, 32, 32, 3), dtype=np.uint8)\n",
    "y_all = np.zeros(50000, dtype=np.int32)\n",
    "\n",
    "for i in range(5):\n",
    "    batch_file = 'data/cifar-10-batches-py/data_batch_' + str(i + 1)\n",
    "    with open(batch_file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "        X_all[10000 * i:10000 * (i+1)] = data['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        y_all[10000 * i:10000 * (i+1)] = data['labels']\n",
    "\n",
    "num_train, num_valid = 40000, 10000\n",
    "X_train, y_train = X_all[:num_train], y_all[:num_train]\n",
    "X_valid, y_valid = X_all[num_train:], y_all[num_train:]\n",
    "\n",
    "x_dim = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cls in enumerate(classes):\n",
    "    cls_ids, = np.where(y_train == i)\n",
    "    draw_ids = np.random.choice(cls_ids, size=10)\n",
    "    \n",
    "    for j, k in enumerate(draw_ids):\n",
    "        plt.subplot(10, 10, j * 10 + i + 1)\n",
    "        plt.imshow(X_train[k])\n",
    "        plt.axis('off')\n",
    "        if j == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pomocné funkce\n",
    "\n",
    "Jelikož budeme pracovat s dávkami, nikoliv jednotlivými vektory, přizpůsobíme tomu všechny funkce, včetně těch pomocných jako je např. `preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_batch):\n",
    "    \"\"\"\n",
    "    funkce odecte prumernou hodnotu pixelu pro kazdy kanal\n",
    "    \n",
    "    vstup:\n",
    "        rgb_batch ... N x H x W x C matice davky vstupnich obrazku\n",
    "    \n",
    "    vystup:\n",
    "        out ... N x H x W x C matice davky upravenych obrazku\n",
    "    \"\"\"\n",
    "    # prevedeni do rozsahu 0..1\n",
    "    out = rgb_batch / 255.\n",
    "    \n",
    "    # prumer pres jednotlive kanaly pro kazdy obrazek --> `m` bude matice N x C\n",
    "    m = np.mean(out, axis=(1, 2))\n",
    "    \n",
    "    # abychom mohli odecist broadcastingem, pridame do `m` dve dimenze\n",
    "    out -= m[:, None, None, :]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(loss, acc=None, avg_range=0):\n",
    "    \"\"\"\n",
    "    vykresli prubeh lossu a prip. do stejneho grafu zakresli i prubeh acc\n",
    "    avg_range ... prumeruj hodnoty po tomto poctu (napr. po 10)\n",
    "    \"\"\"\n",
    "    if avg_range > 0:\n",
    "        div = avg_range * (len(loss) // avg_range)\n",
    "        loss = np.mean(np.reshape(loss[:div], (-1, avg_range)), axis=1)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    ax1.plot(loss, color=colors[0])\n",
    "    ax1.set_ylabel('loss')\n",
    "\n",
    "    if acc is not None:\n",
    "        if avg_range > 0:\n",
    "            acc = np.mean(np.reshape(acc[:div], (-1, avg_range)), axis=1)\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(acc, color=colors[1])\n",
    "        ax2.set_ylabel('acc')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineární klasifikátor modulárně\n",
    "\n",
    "Základním kamenem lineární klasifikace je lineární, či technicky vzato správně afinní, operace $z=Wx+b$. Z uvedeného vztahu se pak odvozuje podoba gradientu na váhy\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z}x^\\top$$\n",
    "Jak jsme si ukázali v přednášce, vyjádřením této operace jako vrstvy s dopředným a zpětným průchodem je možné výpočty libovolně řetězit a kombinovat, což usnadňuje sestavení vícevrstvých hlubokých sítí. \n",
    "\n",
    "Jako první tedy modularizujeme lineární vrstvu, tj. zadefinujeme dvě metody, jednu pro dopředný průchod a druhou pro zpětný.\n",
    "1. Dopředný průchod vrátí $z$ dle výše uvedeného vzorce a pro efektivní výpočty uloží některé proměnné do paměti.\n",
    "2. Zpětný průchod přijme nějaký příchozí gradient (např. z nadřazené vrstvy či lossu), spočítá gradienty na svoje parametry $W$ a $b$ a vstup $x$ a vrátí k případné další propagaci výpočetním grafem.\n",
    "\n",
    "*Pozn.:* Všechny funkce navrhneme tak, aby pracovaly s dávkami vzorků (mini-batch), nikoliv pro samostatné vektory. Změna se projeví pouze tak, že se gradienty pro každý vzorek sečtou. Pokud např. $z_1=Wx_1 + b$ a $z_2=Wx_2 + b$, tj. mezi oběma výpočty se nezměnily hodnoty $W$ a $b$, pak gradient $$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z_1}x_1^\\top + \\frac{\\partial L}{\\partial z_2}x_2^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self._cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim, init_coef=1e-3):\n",
    "        \"\"\"\n",
    "        Linearni (afinni) vrstva\n",
    "        \n",
    "        parametry:\n",
    "            w ... matice vah\n",
    "            b ... bias vektor\n",
    "        \n",
    "        vstup:\n",
    "            input_dim ... rozmer vstupnich dat\n",
    "            output_dim ... rozmer vystupniho linearniho skore\n",
    "            init_coef ... std. ochylka pro inicicializaci vah\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.params = {\n",
    "            'w': init_coef * np.random.randn(input_dim, output_dim),\n",
    "            'b': np.zeros(output_dim),\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        dopredny pruchod linearni vrstvou\n",
    "\n",
    "        vstup:\n",
    "            x ... N x D matice vstupu\n",
    "\n",
    "        vystup:\n",
    "            s ... N x H vystupni linearni skore\n",
    "        \"\"\"\n",
    "        \n",
    "        w, b = self.params['w'], self.params['b']\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        s = ...\n",
    "        \n",
    "        self._cache = ...\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def backward(self, ds):\n",
    "        \"\"\"\n",
    "        zpetny pruchod linearni vrstvou\n",
    "\n",
    "        vstup:\n",
    "            ds ... N x H prichozi gradient z nadrazene vrstvy\n",
    "\n",
    "        vystup:\n",
    "            dx    ... N x D gradient na vstup\n",
    "            grads ... slovnik mapujici jmena parametru na jejich gradienty\n",
    "        \"\"\"\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        ... = self._cache\n",
    "        dx = ...\n",
    "        dw = ...\n",
    "        db = ...\n",
    "\n",
    "        return dx, {'w': dw, 'b': db}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient check\n",
    "\n",
    "Abychom si ověřili, že obě funkce dělají stkuečně to, co mají, porovnáme vypočtené gradienty se \"skutečnými\" hodnotami zjištěnými numerickou diferencí. Ta nevyužívá žádné vzorce, pouze zkoumá, jak se výstup mění v závislosti na malých změnách jednotlivých vstupů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Prevzato z Standford cs231n.\n",
    "    \n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\"\n",
    "    Prevzato z Standford cs231n.\n",
    "    \n",
    "    returns relative error\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_gradients(model, x, dout):\n",
    "    \"\"\"\n",
    "    Funkce vyzkousi dopredny a zpetny pruchod pro kazdy parametr zadaneho modelu\n",
    "    a zkontroluje gradienty vuci numericke diferenci\n",
    "    \n",
    "    model ... objekt tridy Layer; musi implementovat metody forward, forward_func, backward, backward_func\n",
    "    x ... N x D vstupni matice dat\n",
    "    dout ... matice N x H gradientu na vystup site\n",
    "    \"\"\"\n",
    "    \n",
    "    # vypocti gradienty analyticky (potencialne s bugy)\n",
    "    out = model.forward(x)\n",
    "    dx, grads = model.backward(dout)\n",
    "    \n",
    "    # numericky gradient (diference) na vstup (je vzdy az na toleranci spravne)\n",
    "    dx_num = eval_numerical_gradient_array(model.forward, x, dout)\n",
    "    print('dx error: ', rel_error(dx, dx_num))\n",
    "    \n",
    "    # numericky gradient na parametry modelu\n",
    "    for name in model.params:\n",
    "        dp_num = eval_numerical_gradient_array(lambda p: model.forward(x), model.params[name], dout)\n",
    "        print('d{} error: '.format(name), rel_error(grads[name], dp_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zkusebni instance modelu\n",
    "dense_layer = Dense(5, 3)\n",
    "\n",
    "# toy vstupni data\n",
    "x = np.random.randn(10, 5)\n",
    "\n",
    "# \"simulace\" gradientu na vystup\n",
    "dout = np.random.randn(10, 3)\n",
    "\n",
    "# porovnani; rel. chyba gradientu by mela byt velmi mala, radove 1e-10\n",
    "check_gradients(dense_layer, x, dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax cross entropy\n",
    "\n",
    "Kritérium je svým způsobem \"terminální\" vrstva, kde výpočet končí. Hodnota lossu určí, jak moc je výstup ze sítě (vstup do lossu) špatný porovnáním se správnou hodnotou. Druhým krokem je vrátit gradient na svůj vstup (tedy výstup sítě), který říká, co a jak změnit. Jak si tento gradient síť dále přebere a propaguje, už \"je její starost\". Koncepčním rozdílem oproti vnitřním vrstvám sítě je, že loss nepříjímá žádný gradient \"svrchu\" a nedělí se na dopředný a zpětný průchod - vše je v jednom.\n",
    "\n",
    "Výstup logistické regrese je definován jako\n",
    "$$p_c = \\frac{\\exp{s_c}}{\\sum_{i=0}^{C-1}\\exp{s_i}}$$\n",
    "\n",
    "kde $s_c = w_c^\\top x$ je lineární skóre, $w_c$ je $c$-tý řádek matice vah $W$. Pravděpodobnosti $p_c$ pak vstupují do křížové entropie, která je porovná se správným rozložením (samé nuly kromě jedné jedničky pro správnou třídu) dle vztahu\n",
    "$$L = -\\sum_{c=0}^{C-1}\\left(\\boldsymbol{1}(c=y)\\log p_c\\right) = -\\log p_y$$\n",
    "\n",
    "Jelikož je softmax monotónní operace, neboli pořadí prvků dle jejich velikosti se po provedení nemění, nění $p_c$ v praxi potřeba počítat. Pokud bude např. $s_2 = w_c^\\top x > s_5 = w_c^\\top x$, pak i $p_2 > p_5$, a tak můžeme predikovat už na základě skóre $s = Wx + b$ zjištením prvku s max. hodnotou.\n",
    "\n",
    "Loss funkci proto zadefinujeme tak, že softmax bude její součástí. Výhodou tohoto přístupu navíc je, že budeme moci snadno vyměnit loss např. za SVM hinge, které rovněž přijímá skóre, nikoliv pravděpodobnosti. Zpětný průchod, neboli gradient na vektor skóre $s$ je\n",
    "$$\\frac{\\partial L}{\\partial s_c} = \\log p_c - \\boldsymbol{1}(c=y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(score, y, average=True):\n",
    "    \"\"\"\n",
    "    spocita softmax krizovou entropii pro vstupni skore a spravne tridy y\n",
    "    \n",
    "    vstup:\n",
    "        score ... N x H matice skore, n-ty radek je skore pro n-ty obrazek\n",
    "        y ... int vektor o rozmeru N, kde n-ty prvek znaci index spravne tridy n-teho obrazku\n",
    "        average ... pokud True, pak vydeli loss poctem vzorku (zprumeruje)\n",
    "    \n",
    "    vystup:\n",
    "        loss ... hodnota lossu\n",
    "        dscore ... N x H matice gradientu na vstup\n",
    "    \"\"\"\n",
    "    \n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    #################################################################\n",
    "    prob = ...\n",
    "    loss = ...\n",
    "    \n",
    "    if average:\n",
    "        loss /= score.shape[0]\n",
    "    \n",
    "    dscore = prob.copy()\n",
    "    dscore[range(score.shape[0]), y] -= 1.\n",
    "    \n",
    "    return loss, dscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dim = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]\n",
    "softmax_model = Dense(x_dim, len(classes))\n",
    "\n",
    "loss_history = []\n",
    "l2_history = []\n",
    "acc_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trénování\n",
    "\n",
    "Jak jsme si ukázali v prvním cvičení, nejrozšířenějším způsobem učení neuronových sítí je iterativní minimimalizace metodou stochastic gradient descent (SGD). Po malých dávkách síť vždy predikuje výstup, ten se nějakým kritériem (loss) porovná se správnou hodnotou (ground truth) a na základě tohoto rozdílu se upraví vnitřní parametry modelu tak, aby příště výstup co možná nejlépe odpovídal anotaci. Neustále se tedy opakuje:\n",
    "1. navzorkování dávky dat (minibatch)\n",
    "2. dopedný průchod (výstupní skóre)\n",
    "3. vyhocení kritéria (loss)\n",
    "3. zpětný průchod (gradient na parametry)\n",
    "4. update parametrů\n",
    "\n",
    "Jelikož trénování téměř libovolného dopředného modelu pro klasifikaci lze provést tímto způsobem, zabalíme celý proces do metody, které vždy předáme jen části, které se mohou pro různé modely lišit. Nebudeme muset proto psát celý cyklus pro každý model zvlášť."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_sgd(model, loss_func, X_data, y_data, batch_size=20, num_iters=None,\n",
    "             learning_rate=1e-4, l2_reg=0.1, permute=True, print_every=1000):\n",
    "    \"\"\"\n",
    "    Trenovani dopredneho modelu metodou minibatch SGD\n",
    "    \n",
    "    vstup:\n",
    "        model     ... objekt tridy Layer; musi implementovat metody forward a backward\n",
    "        loss_func ... funkce pocitajici hodnotu kriteria - prijima skore a labely y,\n",
    "                      vraci loss + gradient na skore (dscore)\n",
    "        X_data    ... N x D matice trenovacich dat\n",
    "        y_data    ... N vektor labelu (int)\n",
    "        num_iters ... pokud None, pak se dopocita automaticky tak, aby jedno zavolani fce = jedna epocha\n",
    "    \"\"\"\n",
    "    \n",
    "    # V minulem cviceni jsme vybirali vzorky do batche nahodne s opakovanim,\n",
    "    # neboli behem jedne epochy se mohly nektere obrazky opakovat, zatimco\n",
    "    # na jine se vubec nedostalo. Zde budeme prochazet obrazky opet v nahodnem\n",
    "    # poradi, ovsem tak, ze za jednu epochu uvidime kazdy obrazek prave jednou.\n",
    "    # Toho docilime tak, ze data pred pruchodem nahodne zprehazime.\n",
    "    if permute:\n",
    "        perm = np.random.permutation(X_data.shape[0])\n",
    "    else:\n",
    "        perm = np.arange(X_data.shape[0], dtype=np.int)\n",
    "    \n",
    "    if num_iters is None:\n",
    "        num_iters = X_data.shape[0] // batch_size\n",
    "\n",
    "    for n in range(num_iters):\n",
    "        # nacti dalsi batch\n",
    "        batch_ids = perm[n * batch_size:(n + 1) * batch_size]\n",
    "        x = preprocess(X_data[batch_ids]).reshape(-1, x_dim)\n",
    "        y = y_data[batch_ids]\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        # dopredny pruchod\n",
    "        score = ...\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        # softmax cross entropy\n",
    "        loss, dscore = ...\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        # zpetny pruchod\n",
    "        dx, grads = ...\n",
    "\n",
    "        # regularizace (l2_val vubec neni potreba pocitat, zde jen pro ilustraci)\n",
    "        l2_val = 0.\n",
    "        for name, par in model.params.items():\n",
    "            if name.lower().startswith('w'):\n",
    "                l2_val += np.sum(par ** 2.) / x.shape[0]\n",
    "                grads[name] += 2. * learning_rate * par\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        # update parametru\n",
    "        for name, par in model.params.items():\n",
    "            ...\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        # vyhodnotime presnost\n",
    "        acc = ...\n",
    "        acc_history.append(acc)\n",
    "                \n",
    "        # cas od casu vypis, jak se dari\n",
    "        if (n + 1) % print_every == 0:\n",
    "            print('{}/{}: loss={:.3f}, l2={:.3e}, acc={:.3f}'.format(n + 1, num_iters, loss, l2_val, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loss_func = lambda score, y: softmax_cross_entropy(score, y)\n",
    "\n",
    "for ep in range(20):\n",
    "    train_sgd(softmax_model, loss_func, X_train, y_train, print_every=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(loss_history, acc_history, avg_range=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validace\n",
    "\n",
    "Podobně jako trénování, validace je také pokaždé stejný proces, pouze s jinými součástmi. Rovněž ji tedy zabalíme do metody,  které předáme model (tentokrát pouze dopředný průchod) a data, na kterých se má ověřit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model, X_data, y_data, print_every=1000):\n",
    "    \"\"\"\n",
    "    Validace modelu\n",
    "    \n",
    "    vstup:\n",
    "        model     ... objekt tridy Layer; musi implementovat metody forward a backward\n",
    "        X_data    ... N x D matice trenovacich dat\n",
    "        y_data    ... N vektor labelu (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_correct = 0\n",
    "    \n",
    "    for n in range(X_data.shape[0]):   \n",
    "        # ziskame data (jeden vzorek)\n",
    "        xn = preprocess(X_data[None, n]).ravel()\n",
    "        yn = y_data[n]\n",
    "\n",
    "        # pouze dopredny pruchod\n",
    "        # pozn.: funkce upravena pro minibatche, ale xn je zde pouze jeden vektor,\n",
    "        # musime proto pridat jednu dimenzi navic --> minibatch o velikosti 1\n",
    "        score = model.forward(xn[None, ...])\n",
    "        \n",
    "        pred = score.argmax()\n",
    "        if pred == yn:\n",
    "            num_correct += 1\n",
    "        \n",
    "        # cas od casu vypis, jak se dari\n",
    "        if (n + 1) % print_every == 0:\n",
    "            print('{}/{}: acc={:.3f}'.format(n + 1, X_data.shape[0], num_correct / (n + 1)))\n",
    "\n",
    "    print('val accuracy: {}/{} = {:.1f} %'.format(num_correct, X_data.shape[0], 100. * num_correct / X_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "#################################################################\n",
    "validate(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vícevrstvý perceptron\n",
    "\n",
    "Teď, když máme moduralizovanou lineární vrstvu, můžeme si jich vytvořit více a začít je za sebe řetězit. K tomu, abychom vytvořili vícevrstvou síť, však potřebujeme mezi jednotlivé lineární vrstvy ještě přidat nějakou nelineární aktivační funkci. Více lineárních vrstev za sebou totiž dohromady tvoří stále pouze lineární funkci, byť se specifickou faktorizací parametrů.\n",
    "\n",
    "Nejjednodušší vícevrstvá síť je dvouvrstvý perceptron (perceptron s jednou skrytou vrstvou). Model je:\n",
    "1. $s_1 = W_1x + b_1$\n",
    "2. $h = f(x)$\n",
    "3. $s_2 = W_2h + b_2$\n",
    "kde $f$ je nějaká nelinearita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid funkce\n",
    "\n",
    "Historicky populární volbou nelinearity vycházející z biologického modelu neuronů je sigmoida. Vypadá následovně:\n",
    "\n",
    "\n",
    "**dopředný průchod**\n",
    "$$h = \\frac{1}{1 + \\exp(-s)}$$\n",
    "\n",
    "**zpětný průchod**\n",
    "$$\\frac{\\partial L}{\\partial s} = \\frac{\\partial L}{\\partial h} h (1 - h)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, s):\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        h = ...\n",
    "        self._cache = ...\n",
    "        return h\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        ...\n",
    "        ds = ...\n",
    "        return ds, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overime na male davce\n",
    "s = np.random.randn(10, 5)\n",
    "\n",
    "# \"simulace\" prichoziho gradientu, ktery ma vrstva dale propagovat\n",
    "dh = np.random.randn(10, 5)\n",
    "\n",
    "# instance vrstvy\n",
    "sigmoid_layer = Sigmoid()\n",
    "\n",
    "# jelikoz sigmoida nema zadne parametry, kontrolovat se bude pouze gradient na vstup\n",
    "check_gradients(sigmoid_layer, s, dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definice modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(Layer):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, init_coef=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d1 = Dense(input_dim, hidden_dim, init_coef=init_coef)\n",
    "        self.sigm = Sigmoid()\n",
    "        self.d2 = Dense(hidden_dim, output_dim, init_coef=init_coef)\n",
    "        \n",
    "        self.params = {\n",
    "            'w1': self.d1.params['w'],\n",
    "            'b1': self.d1.params['b'],\n",
    "            'w2': self.d2.params['w'],\n",
    "            'b2': self.d2.params['b'],\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        s1 = ...\n",
    "        h = ...\n",
    "        s2 = ...\n",
    "        return s2\n",
    "    \n",
    "    def backward(self, ds2):\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        #################################################################\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        \n",
    "        grads = {\n",
    "            'w1': grads1['w'],\n",
    "            'b1': grads1['b'],\n",
    "            'w2': grads2['w'],\n",
    "            'b2': grads2['b'],\n",
    "        }\n",
    "        return dx, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overime na male davce\n",
    "x = np.random.randn(20, 10)\n",
    "\n",
    "# \"simulace\" prichoziho gradientu, ktery ma vrstva dale propagovat\n",
    "ds2 = np.random.randn(20, 3)\n",
    "\n",
    "# instance vrstvy\n",
    "two_layer_perc = TwoLayerPerceptron(10, 5, 3)\n",
    "\n",
    "# jelikoz sigmoida nema zadne parametry, kontrolovat se bude pouze gradient na vstup\n",
    "check_gradients(two_layer_perc, x, ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dim = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]\n",
    "hidden_dim = 200\n",
    "\n",
    "two_layer_perc = TwoLayerPerceptron(x_dim, hidden_dim, len(classes))\n",
    "\n",
    "loss_history = []\n",
    "l2_history = []\n",
    "acc_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trénování"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "#################################################################\n",
    "for ep in range(...):\n",
    "    train_sgd(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(loss_history, acc_history, avg_range=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "#################################################################\n",
    "validate(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfigurovatelný model vícevrstvého perceptronu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU nelinearita\n",
    "\n",
    "V současnosti je jednou z nejpopulárnějších nelinearit ReLU. Má podobu:\n",
    "\n",
    "**dopředný průchod**\n",
    "$$h = \\max(0, s)$$\n",
    "\n",
    "**zpětný průchod**\n",
    "$$\\frac{\\partial L}{\\partial s} = \\frac{\\partial L}{\\partial h} \\boldsymbol{1}(s \\ge 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, s):\n",
    "        self._cache = s.copy(),\n",
    "        return np.maximum(0., s)\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        s, = self._cache\n",
    "        ds = dh.copy()\n",
    "        ds[s <= 0.] = 0.\n",
    "        return ds, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient check\n",
    "\n",
    "Jelikoz ReLU není diferencovatelná, nemusí si numerický a analytický gradient vždy odpovídat. Chyba vznikne, pokud je ve vstupu $s$ nějaká hodnota blíže nule než je krok $\\Delta_s$ pro numerickou diferenci. Např. centrální diference v bodě 0.001 s krokem 0.01 je\n",
    "$$\\frac{ReLU(0.001 + 0.01) - ReLU(0.001 - 0.01)}{2\\cdot 0.01} = \\frac{0.011 - 0}{0.02} = 0.55$$\n",
    "zatímco analyticky je derivace v bodě $0.001$ rovna přesně $1$. Je tedy možné, že se občas objeví velká rel. odchylka gradientů, mnohem častěji pro malé hodnoty vstupu $s$. Napravit to lze např. menším krokem $\\Delta_s$ numerické diference, který by měl být řádově menší než vstup $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overime na male davce\n",
    "s = np.random.randn(10, 5)\n",
    "\n",
    "# \"simulace\" prichoziho gradientu, ktery ma vrstva dale propagovat\n",
    "dh = np.random.randn(10, 5)\n",
    "\n",
    "relu = ReLU()\n",
    "check_gradients(relu, s, dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definice modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardNet(Layer):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        self.params = {}\n",
    "        for i, layer in enumerate(layers):\n",
    "            for name, par in layer.params.items():\n",
    "                self.params[name + str(i)] = par\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dx):\n",
    "        model_grads = {}\n",
    "        \n",
    "        for i, layer in reversed(list(enumerate(self.layers))):\n",
    "            dx, layer_grads = layer.backward(dx)\n",
    "            \n",
    "            for name, dpar in layer_grads.items():\n",
    "                model_grads[name + str(i)] = dpar\n",
    "        \n",
    "        return dx, model_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gradient check\n",
    "\n",
    "# overime na male davce\n",
    "x = np.random.randn(20, 15)\n",
    "\n",
    "# \"simulace\" prichoziho gradientu, ktery ma vrstva dale propagovat\n",
    "dout = np.random.randn(20, 5)\n",
    "\n",
    "layers = [\n",
    "    Dense(15, 10),\n",
    "    Sigmoid(),\n",
    "    Dense(10, 8),\n",
    "    ReLU(),\n",
    "    Dense(8, 5),\n",
    "]\n",
    "feed_forward_net = FeedForwardNet(layers)\n",
    "check_gradients(feed_forward_net, x, dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Trénování\n",
    "\n",
    "Natrénujte co nejlepší model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE JE PROSTOR PRO VASI KREATIVITU\n",
    "#################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
